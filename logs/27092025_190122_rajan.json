{
  "source": "streamlit",
  "current_login": "27092025_190122_rajan",
  "user_name": "rajan",
  "user_prompt": "what's the difference between the self-attention and multi-head attention?",
  "user_uploads": "attention_is_all_you_need.pdf",
  "answer": "Multi-head attention differs from a single attention head (which self-attention can be an instance of) primarily in its ability to jointly attend to information from different representation subspaces at different positions. A single attention head, by averaging, inhibits this capability.\n\nHere's a breakdown of the differences based on the context:\n\n**Multi-Head Attention:**\n1.  **Mechanism:** It consists of several attention layers (referred to as \"heads\") running in parallel.\n2.  **Projections:** For each head, the queries (Q), keys (K), and values (V) undergo independent linear projections into `dk` and `dv` dimensions, respectively.\n3.  **Parallel Execution:** An attention function (e.g., Scaled Dot-Product Attention) is performed in parallel on each of these projected versions of Q, K, and V, yielding `dv`-dimensional output values for each head.\n4.  **Concatenation and Final Projection:** The output values from all heads are concatenated and then projected once again to produce the final output.\n5.  **Benefit:** This architecture allows the model to capture information from various representation subspaces and different positions simultaneously.\n\n**Self-Attention (as implied in contrast to Multi-Head):**\n1.  **Mechanism:** While not explicitly defined as a single head, the text implies that Multi-Head Attention addresses limitations of a \"single attention head.\" When Q, K, and V come from the same sequence, the attention mechanism is referred to as self-attention.\n2.  **Limitation:** A single attention head, through averaging, inhibits the model's ability to jointly attend to information from different representation subspaces at different positions, which is a key advantage of Multi-Head Attention.\n3.  **General Role:** Self-attention layers are used for mapping one variable-length sequence of symbol representations to another sequence of equal length. They offer benefits in terms of computational complexity, parallelization, path length between long-range dependencies, and potentially more interpretable models.\n\nIn essence, Multi-Head Attention is an advanced form of attention that leverages multiple parallel attention \"heads\" to overcome the limitations of a single attention head, thereby allowing the model to learn richer and more diverse relationships within the data.\n\n**Sources:**\n*   \"Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\"\n*   \"MultiHead(Q, K, V) = Concat(head1, ..., headh)WO where headi = Attention(QWQ i , KWK i , VWV i )\"\n*   \"linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional output values. These are concatenated and once again projected, resulting in the final values\"\n*   Figure 2 description: \"Multi-Head Attention consists of several attention layers running in parallel.\"\n*   Section 4 \"Why Self-Attention\" for general characteristics of self-attention layers.",
  "created_at": "2025-09-27T13:31:22.876484"
}